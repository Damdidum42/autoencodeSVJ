{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to JupyROOT 6.12/06\n"
     ]
    }
   ],
   "source": [
    "from autoencodeSVJ import utils, models, trainer\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import keras\n",
    "import keras.backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "import root_numpy as rtnp\n",
    "\n",
    "version_id = 'v4'\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "background tags:\n",
      "\n",
      "down jet: 18999, 18.4%\n",
      "up jet: 38841, 37.6%\n",
      "strange jet: 25377, 24.5%\n",
      "charm jet: 9577, 9.3%\n",
      "bottom jet: 3341, 3.2%\n",
      "gluon jet: 7281, 7.0%\n",
      "\n",
      "signal tags:\n",
      "\n",
      "down jet: 1220, 2.5%\n",
      "up jet: 1866, 3.8%\n",
      "strange jet: 1550, 3.1%\n",
      "charm jet: 3344, 6.8%\n",
      "bottom jet: 38995, 78.8%\n",
      "gluon jet: 2541, 5.1%\n",
      "\n",
      "bottom:\n",
      "\t3.2% (2664) train\n",
      "\t3.3% (677) test\n",
      "\t3.2% (3341) background\n",
      "\t78.8% (38995) signal\n",
      "up:\n",
      "\t37.5% (30997) train\n",
      "\t37.9% (7844) test\n",
      "\t37.6% (38841) background\n",
      "\t3.8% (1866) signal\n",
      "down:\n",
      "\t18.4% (15192) train\n",
      "\t18.4% (3807) test\n",
      "\t18.4% (18999) background\n",
      "\t2.5% (1220) signal\n",
      "strange:\n",
      "\t24.6% (20367) train\n",
      "\t24.2% (5010) test\n",
      "\t24.5% (25377) background\n",
      "\t3.1% (1550) signal\n",
      "charm:\n",
      "\t9.3% (7667) train\n",
      "\t9.2% (1910) test\n",
      "\t9.3% (9577) background\n",
      "\t6.8% (3344) signal\n",
      "gluon:\n",
      "\t7.1% (5845) train\n",
      "\t6.9% (1436) test\n",
      "\t7.0% (7281) background\n",
      "\t5.1% (2541) signal\n"
     ]
    }
   ],
   "source": [
    "# get h5 datasets\n",
    "data_path = \"data/background/*_data.h5\"\n",
    "signal_path = \"data/signal/*_data.h5\"\n",
    "\n",
    "data, data_tags, data_jets = utils.load_all_data(data_path, \"background\")\n",
    "signal, signal_tags, signal_jets = utils.load_all_data(signal_path, \"signal\")\n",
    "\n",
    "split = 0.2\n",
    "\n",
    "train, test = data.train_test_split(split, SEED)\n",
    "utils.compare_tags([train, test, data, signal])\n",
    "\n",
    "data_raw = data.cdrop(\"*Flavor\")\n",
    "signal_raw = signal.cdrop(\"*Flavor\")\n",
    "train_raw = train.cdrop(\"*Flavor\")\n",
    "test_raw = test.cdrop(\"*Flavor\")\n",
    "\n",
    "norm_args = {\"norm_type\": \"StandardScaler\"}\n",
    "train_norm, test_norm = train_raw.norm(**norm_args), train_raw.norm(test_raw, **norm_args)\n",
    "data_norm, signal_norm = train_raw.norm(data_raw, **norm_args), train_raw.norm(signal_raw, **norm_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make autoencoder\n",
    "Here we have some characterizing parameters for the VAE which can be messed with to try and acheive better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "l1_amount = None          # l1 regularization amount\n",
    "KL_loss_weight = 1.8    # according to olmo's paper\n",
    "input_dim = 7              # num. HLF\n",
    "interm_dim = 20            # num. interm nodes\n",
    "neck_dim = 4             # num. latent variables\n",
    "kernel_max_norm = None     # kernel max norm amount\n",
    "\n",
    "name = \"bn_{0}_{1}\".format(neck_dim, version_id)\n",
    "repo_head = utils.get_repo_info()['head']\n",
    "model_path = repo_head + \"/autoencode/data/training_runs/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss functions and helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "import keras\n",
    "\n",
    "def isrlu(args, min_value = 5e-3):\n",
    "    \"\"\"ranges from (0, +inf), with relu behavior at x > 0 and exponential decay at x < 0\"\"\"\n",
    "    return 1. + min_value + K.tf.where(K.tf.greater(args, 0), args, K.tf.divide(args, K.sqrt(1+K.square(args))))\n",
    "\n",
    "def sample_from_gauss(args):\n",
    "    \"\"\"reparam trick for VAE\"\"\"\n",
    "    z_mean, z_sigma = args\n",
    "    epsilon = K.random_normal(shape=(K.shape(z_mean)[0], K.shape(z_mean)[1]), mean=0., stddev=1.)\n",
    "    return z_mean + z_sigma * epsilon\n",
    "\n",
    "def make_zero(x):\n",
    "    \"\"\"lambda zero function, for forward pass of priors from x variable\"\"\"\n",
    "    return 0.*x\n",
    "\n",
    "def KL_loss(mu, sigma, mu_prior, sigma_prior):\n",
    "    \"\"\"KL loss from mu, sigma, and their priors\"\"\"\n",
    "    kl_loss = K.tf.multiply(K.square(sigma), K.square(sigma_prior))\n",
    "    kl_loss += K.square(K.tf.divide(mu_prior - mu, sigma_prior))\n",
    "    kl_loss += K.log(K.tf.divide(sigma_prior, sigma)) -1\n",
    "    return 0.5 * K.sum(kl_loss, axis=-1)\n",
    "\n",
    "def reconstruction_probability(x, mu, sigma):\n",
    "    \"\"\"calculate gaussian -log likelihood across all variables and sum\"\"\"\n",
    "    z_score = K.tf.divide(x - mu, sigma)\n",
    "    single = - K.log(1./(sigma*np.sqrt(2.*np.pi)))/np.log(np.e) + 0.5*K.square(z_score)\n",
    "    return K.sum(single, axis=-1)\n",
    "\n",
    "class recon_prob_wrapper_layer(keras.layers.Layer):\n",
    "    \"\"\"Wrapper for recon. prob. function. \n",
    "    Outputs reconstruction probability of gaussians for vae\n",
    "    \"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        self.is_placeholder = True\n",
    "        super(recon_prob_wrapper_layer, self).__init__(**kwargs)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        x, mu, sigma = inputs\n",
    "        return reconstruction_probability(x, mu, sigma)\n",
    "    \n",
    "class KL_loss_wrapper_layer(keras.layers.Layer):\n",
    "    \"\"\"Wrapper for KL_loss function. \n",
    "    Layer which outputs KL loss according the the function defined above\n",
    "    \"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        self.is_placeholder = True\n",
    "        super(KL_loss_wrapper_layer, self).__init__(**kwargs)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        mu, sigma, mu_prior, sigma_prior = inputs\n",
    "        return KL_loss(mu, sigma, mu_prior, sigma_prior)\n",
    "    \n",
    "def Identity(y_train, vae_output):\n",
    "    \"\"\"identity loss, takes into account ONLY the VAE output (vae output should actually be the \n",
    "    calculated loss functions!)\"\"\"\n",
    "    return K.mean(vae_output)\n",
    "\n",
    "custom_objects = {\n",
    "    \"isrlu\": isrlu,\n",
    "    \"sample_from_gauss\": sample_from_gauss,\n",
    "    \"make_zero\": make_zero,\n",
    "    \"KL_loss\": KL_loss,\n",
    "    \"reconstruction_probability\": reconstruction_probability,\n",
    "    \"recon_prob_wrapper_layer\": recon_prob_wrapper_layer,\n",
    "    \"KL_loss_wrapper_layer\": KL_loss_wrapper_layer,\n",
    "    \"Identity\": Identity,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define layers\n",
    "Define layers to setup layer architecture for later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "import keras\n",
    "from keras.constraints import max_norm\n",
    "\n",
    "# define layers\n",
    "input_layer = keras.layers.Input(shape=(input_dim,), name=\"input\")\n",
    "\n",
    "intermediate_layer_enc1 = keras.layers.Dense(interm_dim, name=\"hidden_enc1\")\n",
    "intermediate_layer_enc2 = keras.layers.Dense(interm_dim, activation='relu', kernel_constraint=max_norm(kernel_max_norm), name='hidden_enc2')\n",
    "\n",
    "z_mu_layer = keras.layers.Dense(neck_dim, name='z_mu')\n",
    "z_sigma_pre_layer = keras.layers.Dense(neck_dim, name=\"z_sigma_pre\")\n",
    "z_sigma_layer = keras.layers.Lambda(isrlu, name=\"z_sigma\")\n",
    "\n",
    "z_latent_layer = keras.layers.Lambda(sample_from_gauss, name=\"latent_sampling\")\n",
    "z_input_layer = keras.layers.Input(shape=(neck_dim,), name=\"z_input_layer\")\n",
    "\n",
    "intermediate_layer_dec1 = keras.layers.Dense(interm_dim, name=\"hidden_dec1\", kernel_constraint=max_norm(kernel_max_norm))\n",
    "intermediate_layer_dec2 = keras.layers.Dense(interm_dim, name=\"hidden_dec2\")\n",
    "\n",
    "output_sigma_pre_layer = keras.layers.Dense(input_dim, name=\"output_sigma_pre\", activation=\"linear\")\n",
    "output_sigma_layer = keras.layers.Lambda(isrlu, name=\"output_sigma\")\n",
    "output_mu_layer = keras.layers.Dense(input_dim, name=\"output_mu\", activation=\"linear\")\n",
    "\n",
    "# prior layers for KL divergence\n",
    "fixed_input_layer = keras.layers.Lambda(make_zero, name=\"fixed_input\")\n",
    "prior_layer = keras.layers.Dense(1, kernel_initializer='zeros', bias_initializer='ones', trainable=False, name='prior')\n",
    "prior_mu_layer = keras.layers.Dense(neck_dim, kernel_initializer='zeros', bias_initializer='zeros', trainable=True, name='prior_mu')\n",
    "prior_sigma_pre_layer = keras.layers.Dense(neck_dim, kernel_initializer='zeros', bias_initializer='ones', trainable=True, name='prior_sigma_pre')\n",
    "prior_sigma_layer = keras.layers.Lambda(isrlu, name='prior_sigma')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              (None, 7)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "hidden_enc1 (Dense)             (None, 20)           160         input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "hidden_enc2 (Dense)             (None, 20)           420         hidden_enc1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "z_sigma_pre (Dense)             (None, 4)            84          hidden_enc2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "z_mu (Dense)                    (None, 4)            84          hidden_enc2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "z_sigma (Lambda)                (None, 4)            0           z_sigma_pre[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "latent_sampling (Lambda)        (None, 4)            0           z_mu[0][0]                       \n",
      "                                                                 z_sigma[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "fixed_input (Lambda)            (None, 7)            0           input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "hidden_dec1 (Dense)             (None, 20)           100         latent_sampling[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "prior (Dense)                   (None, 1)            8           fixed_input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "hidden_dec2 (Dense)             (None, 20)           420         hidden_dec1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "prior_sigma_pre (Dense)         (None, 4)            8           prior[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "output_sigma_pre (Dense)        (None, 7)            147         hidden_dec2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "prior_mu (Dense)                (None, 4)            8           prior[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "prior_sigma (Lambda)            (None, 4)            0           prior_sigma_pre[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "output_mu (Dense)               (None, 7)            147         hidden_dec2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "output_sigma (Lambda)           (None, 7)            0           output_sigma_pre[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "KL_loss_layer (KL_loss_wrapper_ [(None, 4), (None, 4 0           z_mu[0][0]                       \n",
      "                                                                 z_sigma[0][0]                    \n",
      "                                                                 prior_mu[0][0]                   \n",
      "                                                                 prior_sigma[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "reconstructed_probability (reco [(None, 7), (None, 7 0           input[0][0]                      \n",
      "                                                                 output_mu[0][0]                  \n",
      "                                                                 output_sigma[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 1,586\n",
      "Trainable params: 1,578\n",
      "Non-trainable params: 8\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## INPUT to encoder\n",
    "aux = intermediate_layer_enc1(input_layer)   # first intermediate layer\n",
    "aux = intermediate_layer_enc2(aux)\n",
    "\n",
    "z_mu = z_mu_layer(aux)                    # z mu layer\n",
    "z_sigma = z_sigma_layer(z_sigma_pre_layer(aux)) # z sigma layer (with correction)\n",
    "z_latent = z_latent_layer([z_mu, z_sigma])      # z latent layer (draw from latent distribution, zmu/zsigma)\n",
    "\n",
    "## INPUT to decoder\n",
    "aux = intermediate_layer_dec1(z_latent) # z_input_layer if you want a decoder\n",
    "aux = intermediate_layer_dec2(aux)\n",
    "out_mu = output_mu_layer(aux)\n",
    "out_sigma = output_sigma_layer(output_sigma_pre_layer(aux))\n",
    "## OUTPUT to decoder\n",
    "\n",
    "# reparameterization trick!\n",
    "fixed_input = fixed_input_layer(input_layer)        # fixed input layer (no training)\n",
    "prior = prior_layer(fixed_input)                    # prior layer (one node, all constant)\n",
    "prior_mu = prior_mu_layer(prior)                    # mu layer (all ones, trainable)\n",
    "prior_sigma = prior_sigma_layer(prior_sigma_pre_layer(prior)) # sigma layer (all geq zero, trainable)\n",
    "\n",
    "\n",
    "# # make encoder/decoder models\n",
    "# encoder = keras.models.Model(inputs=input_layer, outputs=z_latent, name=\"encoder\")\n",
    "# decoder = keras.models.Model(inputs=z_input_layer, outputs=[out_mu, out_sigma], name=\"decoder\")\n",
    "\n",
    "# reconstruction probability: create from input layer, as well as outputs of decoder (mu/sigma out)\n",
    "reco_prob_out = recon_prob_wrapper_layer(name=\"reconstructed_probability\")([input_layer, out_mu, out_sigma])\n",
    "# KL loss: create from zed mu/sigma, as well as prior variables\n",
    "KL_loss_out = KL_loss_wrapper_layer(name=\"KL_loss_layer\")([z_mu, z_sigma, prior_mu, prior_sigma])\n",
    "\n",
    "# autoencoder made from x-input to loss outputs\n",
    "vae = keras.models.Model(inputs=input_layer, outputs=[KL_loss_out, reco_prob_out])\n",
    "\n",
    "vae.compile(optimizer=\"adam\", loss=[Identity, Identity], loss_weights=[KL_loss_weight, 1.])\n",
    "vae.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h5_elt 'metric_names' :: creating group 'data' in file 'bn_4_v4.h5'\n",
      "h5_elt 'metric_names' :: creating dataset 'data/metric_names' in file 'bn_4_v4.h5'\n",
      "h5_elt 'training' :: creating group 'params' in file 'bn_4_v4.h5'\n",
      "h5_elt 'training' :: creating dataset 'params/training' in file 'bn_4_v4.h5'\n",
      "h5_elt 'config' :: creating dataset 'params/config' in file 'bn_4_v4.h5'\n"
     ]
    }
   ],
   "source": [
    "instance = trainer.trainer(os.path.join(model_path, name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# vae = instance.load_model(custom_objects=custom_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 82732 samples, validate on 20684 samples\n",
      "Epoch 1/20\n",
      "82732/82732 [==============================] - 2s 22us/step - loss: 81.6447 - KL_loss_layer_loss: 8.0191 - reconstructed_probability_loss: 81.6447 - val_loss: 18.0617 - val_KL_loss_layer_loss: 6.9919 - val_reconstructed_probability_loss: 18.0617\n",
      "Epoch 2/20\n",
      "82732/82732 [==============================] - 1s 13us/step - loss: 15.7176 - KL_loss_layer_loss: 6.6358 - reconstructed_probability_loss: 15.7176 - val_loss: 12.5363 - val_KL_loss_layer_loss: 6.3316 - val_reconstructed_probability_loss: 12.5363\n",
      "Epoch 3/20\n",
      "82732/82732 [==============================] - 1s 13us/step - loss: 12.1058 - KL_loss_layer_loss: 6.1762 - reconstructed_probability_loss: 12.1058 - val_loss: 11.6217 - val_KL_loss_layer_loss: 6.0215 - val_reconstructed_probability_loss: 11.6217\n",
      "Epoch 4/20\n",
      "82732/82732 [==============================] - 1s 12us/step - loss: 11.2606 - KL_loss_layer_loss: 5.9207 - reconstructed_probability_loss: 11.2606 - val_loss: 10.8055 - val_KL_loss_layer_loss: 5.8039 - val_reconstructed_probability_loss: 10.8055\n",
      "Epoch 5/20\n",
      "82732/82732 [==============================] - 1s 12us/step - loss: 10.7137 - KL_loss_layer_loss: 5.7333 - reconstructed_probability_loss: 10.7137 - val_loss: 10.4762 - val_KL_loss_layer_loss: 5.6399 - val_reconstructed_probability_loss: 10.4762\n",
      "Epoch 6/20\n",
      "82732/82732 [==============================] - 1s 12us/step - loss: 10.3259 - KL_loss_layer_loss: 5.5849 - reconstructed_probability_loss: 10.3259 - val_loss: 10.1526 - val_KL_loss_layer_loss: 5.5036 - val_reconstructed_probability_loss: 10.1526\n",
      "Epoch 7/20\n",
      "82732/82732 [==============================] - 1s 13us/step - loss: 10.0313 - KL_loss_layer_loss: 5.4646 - reconstructed_probability_loss: 10.0313 - val_loss: 9.9506 - val_KL_loss_layer_loss: 5.4056 - val_reconstructed_probability_loss: 9.9506\n",
      "Epoch 8/20\n",
      "82732/82732 [==============================] - 1s 12us/step - loss: 9.8672 - KL_loss_layer_loss: 5.3697 - reconstructed_probability_loss: 9.8672 - val_loss: 9.6849 - val_KL_loss_layer_loss: 5.3110 - val_reconstructed_probability_loss: 9.6849\n",
      "Epoch 9/20\n",
      "82732/82732 [==============================] - 1s 12us/step - loss: 9.5774 - KL_loss_layer_loss: 5.2834 - reconstructed_probability_loss: 9.5774 - val_loss: 9.4334 - val_KL_loss_layer_loss: 5.2364 - val_reconstructed_probability_loss: 9.4334\n",
      "Epoch 10/20\n",
      "82732/82732 [==============================] - 1s 12us/step - loss: 9.3507 - KL_loss_layer_loss: 5.2165 - reconstructed_probability_loss: 9.3507 - val_loss: 9.1817 - val_KL_loss_layer_loss: 5.1788 - val_reconstructed_probability_loss: 9.1817\n",
      "Epoch 11/20\n",
      "82732/82732 [==============================] - 1s 13us/step - loss: 9.0826 - KL_loss_layer_loss: 5.1731 - reconstructed_probability_loss: 9.0826 - val_loss: 8.9136 - val_KL_loss_layer_loss: 5.1458 - val_reconstructed_probability_loss: 8.9136\n",
      "Epoch 12/20\n",
      "82732/82732 [==============================] - 1s 12us/step - loss: 8.7586 - KL_loss_layer_loss: 5.1513 - reconstructed_probability_loss: 8.7586 - val_loss: 8.6080 - val_KL_loss_layer_loss: 5.1504 - val_reconstructed_probability_loss: 8.6080\n",
      "Epoch 13/20\n",
      "82732/82732 [==============================] - 1s 12us/step - loss: 8.4388 - KL_loss_layer_loss: 5.1705 - reconstructed_probability_loss: 8.4388 - val_loss: 8.2615 - val_KL_loss_layer_loss: 5.1795 - val_reconstructed_probability_loss: 8.2615\n",
      "Epoch 14/20\n",
      "82732/82732 [==============================] - 1s 12us/step - loss: 8.0824 - KL_loss_layer_loss: 5.2046 - reconstructed_probability_loss: 8.0824 - val_loss: 7.8809 - val_KL_loss_layer_loss: 5.2243 - val_reconstructed_probability_loss: 7.8809\n",
      "Epoch 15/20\n",
      "82732/82732 [==============================] - 1s 12us/step - loss: 7.6786 - KL_loss_layer_loss: 5.2591 - reconstructed_probability_loss: 7.6786 - val_loss: 7.4505 - val_KL_loss_layer_loss: 5.2976 - val_reconstructed_probability_loss: 7.4505\n",
      "Epoch 16/20\n",
      "82732/82732 [==============================] - 1s 12us/step - loss: 7.2298 - KL_loss_layer_loss: 5.3454 - reconstructed_probability_loss: 7.2298 - val_loss: 6.9828 - val_KL_loss_layer_loss: 5.4061 - val_reconstructed_probability_loss: 6.9828\n",
      "Epoch 17/20\n",
      "82732/82732 [==============================] - 1s 12us/step - loss: 6.7398 - KL_loss_layer_loss: 5.4755 - reconstructed_probability_loss: 6.7398 - val_loss: 6.5002 - val_KL_loss_layer_loss: 5.5596 - val_reconstructed_probability_loss: 6.5002\n",
      "Epoch 18/20\n",
      "82732/82732 [==============================] - 1s 12us/step - loss: 6.2371 - KL_loss_layer_loss: 5.6474 - reconstructed_probability_loss: 6.2371 - val_loss: 5.9901 - val_KL_loss_layer_loss: 5.7471 - val_reconstructed_probability_loss: 5.9901\n",
      "Epoch 19/20\n",
      "82732/82732 [==============================] - 1s 12us/step - loss: 5.7352 - KL_loss_layer_loss: 5.8407 - reconstructed_probability_loss: 5.7352 - val_loss: 5.5131 - val_KL_loss_layer_loss: 5.9419 - val_reconstructed_probability_loss: 5.5131\n",
      "Epoch 20/20\n",
      "82732/82732 [==============================] - 1s 12us/step - loss: 5.2500 - KL_loss_layer_loss: 6.0295 - reconstructed_probability_loss: 5.2500 - val_loss: 5.0090 - val_KL_loss_layer_loss: 6.1366 - val_reconstructed_probability_loss: 5.0090\n",
      "EPOCH N: 20 20\n",
      "train_shell :: \n",
      "train_shell :: trained 20 epochs!\n",
      "train_shell :: \n",
      "h5_elt 'loss' :: creating group 'metric_names' in file 'bn_4_v4.h5'\n",
      "h5_elt 'loss' :: creating dataset 'metric_names/loss' in file 'bn_4_v4.h5'\n",
      "h5_elt 'KL_loss_layer_loss' :: creating dataset 'metric_names/KL_loss_layer_loss' in file 'bn_4_v4.h5'\n",
      "h5_elt 'lr' :: creating dataset 'metric_names/lr' in file 'bn_4_v4.h5'\n",
      "h5_elt 'val_KL_loss_layer_loss' :: creating dataset 'metric_names/val_KL_loss_layer_loss' in file 'bn_4_v4.h5'\n",
      "h5_elt 'val_reconstructed_probability_loss' :: creating dataset 'metric_names/val_reconstructed_probability_loss' in file 'bn_4_v4.h5'\n",
      "h5_elt 'reconstructed_probability_loss' :: creating dataset 'metric_names/reconstructed_probability_loss' in file 'bn_4_v4.h5'\n",
      "h5_elt 'val_loss' :: creating dataset 'metric_names/val_loss' in file 'bn_4_v4.h5'\n",
      "finished epoch N 20\n",
      "prev [20]\n",
      "train_shell :: model saved\n",
      "train_shell :: ERROR: IGNORING PASSED PARAMETER 'model'\n",
      "train_shell :: using saved model\n",
      "Train on 82732 samples, validate on 20684 samples\n",
      "Epoch 21/120\n",
      "82732/82732 [==============================] - 2s 18us/step - loss: 12.2833 - KL_loss_layer_loss: 4.2080 - reconstructed_probability_loss: 4.7089 - val_loss: 10.6690 - val_KL_loss_layer_loss: 3.4530 - val_reconstructed_probability_loss: 4.4537\n",
      "Epoch 22/120\n",
      "82732/82732 [==============================] - 1s 12us/step - loss: 9.9706 - KL_loss_layer_loss: 3.2350 - reconstructed_probability_loss: 4.1476 - val_loss: 9.4537 - val_KL_loss_layer_loss: 3.0703 - val_reconstructed_probability_loss: 3.9271\n",
      "Epoch 23/120\n",
      "82732/82732 [==============================] - 1s 12us/step - loss: 9.0793 - KL_loss_layer_loss: 2.9634 - reconstructed_probability_loss: 3.7451 - val_loss: 8.7553 - val_KL_loss_layer_loss: 2.8777 - val_reconstructed_probability_loss: 3.5754\n",
      "Epoch 24/120\n",
      "82732/82732 [==============================] - 1s 12us/step - loss: 8.5151 - KL_loss_layer_loss: 2.8241 - reconstructed_probability_loss: 3.4317 - val_loss: 8.2947 - val_KL_loss_layer_loss: 2.7868 - val_reconstructed_probability_loss: 3.2785\n",
      "Epoch 25/120\n",
      "82732/82732 [==============================] - 1s 13us/step - loss: 8.1108 - KL_loss_layer_loss: 2.7384 - reconstructed_probability_loss: 3.1816 - val_loss: 7.9368 - val_KL_loss_layer_loss: 2.7044 - val_reconstructed_probability_loss: 3.0689\n",
      "Epoch 26/120\n",
      "82732/82732 [==============================] - 1s 13us/step - loss: 7.7985 - KL_loss_layer_loss: 2.6523 - reconstructed_probability_loss: 3.0244 - val_loss: 7.6321 - val_KL_loss_layer_loss: 2.5928 - val_reconstructed_probability_loss: 2.9652\n",
      "Epoch 27/120\n",
      "82732/82732 [==============================] - 1s 12us/step - loss: 7.4658 - KL_loss_layer_loss: 2.5288 - reconstructed_probability_loss: 2.9140 - val_loss: 7.2799 - val_KL_loss_layer_loss: 2.4617 - val_reconstructed_probability_loss: 2.8489\n",
      "Epoch 28/120\n",
      "82732/82732 [==============================] - 1s 12us/step - loss: 7.0582 - KL_loss_layer_loss: 2.3534 - reconstructed_probability_loss: 2.8220 - val_loss: 6.8202 - val_KL_loss_layer_loss: 2.2139 - val_reconstructed_probability_loss: 2.8353\n",
      "Epoch 29/120\n",
      "82732/82732 [==============================] - 1s 11us/step - loss: 6.4883 - KL_loss_layer_loss: 2.0945 - reconstructed_probability_loss: 2.7181 - val_loss: 6.1127 - val_KL_loss_layer_loss: 1.9667 - val_reconstructed_probability_loss: 2.5727\n",
      "Epoch 30/120\n",
      "82732/82732 [==============================] - 1s 12us/step - loss: 5.8246 - KL_loss_layer_loss: 1.8430 - reconstructed_probability_loss: 2.5072 - val_loss: 5.5667 - val_KL_loss_layer_loss: 1.7324 - val_reconstructed_probability_loss: 2.4484\n",
      "Epoch 31/120\n",
      "82732/82732 [==============================] - 1s 12us/step - loss: 5.2756 - KL_loss_layer_loss: 1.6373 - reconstructed_probability_loss: 2.3286 - val_loss: 5.0268 - val_KL_loss_layer_loss: 1.5587 - val_reconstructed_probability_loss: 2.2212\n",
      "Epoch 32/120\n",
      "82732/82732 [==============================] - 1s 12us/step - loss: 4.8046 - KL_loss_layer_loss: 1.4785 - reconstructed_probability_loss: 2.1433 - val_loss: 4.6055 - val_KL_loss_layer_loss: 1.4165 - val_reconstructed_probability_loss: 2.0557\n",
      "Epoch 33/120\n",
      "82732/82732 [==============================] - 1s 12us/step - loss: 4.4016 - KL_loss_layer_loss: 1.3573 - reconstructed_probability_loss: 1.9585 - val_loss: 4.2317 - val_KL_loss_layer_loss: 1.3117 - val_reconstructed_probability_loss: 1.8707\n",
      "Epoch 34/120\n",
      "82732/82732 [==============================] - 1s 12us/step - loss: 4.0727 - KL_loss_layer_loss: 1.2596 - reconstructed_probability_loss: 1.8054 - val_loss: 3.9146 - val_KL_loss_layer_loss: 1.2025 - val_reconstructed_probability_loss: 1.7501\n",
      "Epoch 35/120\n",
      "82732/82732 [==============================] - 1s 12us/step - loss: 3.7755 - KL_loss_layer_loss: 1.1669 - reconstructed_probability_loss: 1.6750 - val_loss: 3.6359 - val_KL_loss_layer_loss: 1.1289 - val_reconstructed_probability_loss: 1.6039\n",
      "Epoch 36/120\n",
      "82732/82732 [==============================] - 1s 12us/step - loss: 3.5034 - KL_loss_layer_loss: 1.0932 - reconstructed_probability_loss: 1.5356 - val_loss: 3.3780 - val_KL_loss_layer_loss: 1.0563 - val_reconstructed_probability_loss: 1.4767\n",
      "Epoch 37/120\n",
      "82732/82732 [==============================] - 1s 12us/step - loss: 3.2595 - KL_loss_layer_loss: 1.0343 - reconstructed_probability_loss: 1.3978 - val_loss: 3.1223 - val_KL_loss_layer_loss: 0.9982 - val_reconstructed_probability_loss: 1.3256\n",
      "Epoch 38/120\n",
      "82732/82732 [==============================] - 1s 12us/step - loss: 3.0424 - KL_loss_layer_loss: 0.9704 - reconstructed_probability_loss: 1.2957 - val_loss: 2.9948 - val_KL_loss_layer_loss: 0.9288 - val_reconstructed_probability_loss: 1.3230\n",
      "Epoch 39/120\n",
      "82732/82732 [==============================] - 1s 11us/step - loss: 2.8254 - KL_loss_layer_loss: 0.9056 - reconstructed_probability_loss: 1.1953 - val_loss: 2.6993 - val_KL_loss_layer_loss: 0.8813 - val_reconstructed_probability_loss: 1.1129\n",
      "Epoch 40/120\n",
      "82732/82732 [==============================] - 1s 12us/step - loss: 2.6535 - KL_loss_layer_loss: 0.8516 - reconstructed_probability_loss: 1.1206 - val_loss: 2.5380 - val_KL_loss_layer_loss: 0.8163 - val_reconstructed_probability_loss: 1.0687\n",
      "Epoch 41/120\n",
      "82732/82732 [==============================] - 1s 12us/step - loss: 2.4553 - KL_loss_layer_loss: 0.7946 - reconstructed_probability_loss: 1.0251 - val_loss: 2.3698 - val_KL_loss_layer_loss: 0.7718 - val_reconstructed_probability_loss: 0.9805\n",
      "Epoch 42/120\n",
      "82732/82732 [==============================] - 1s 12us/step - loss: 2.3050 - KL_loss_layer_loss: 0.7423 - reconstructed_probability_loss: 0.9688 - val_loss: 2.2055 - val_KL_loss_layer_loss: 0.7202 - val_reconstructed_probability_loss: 0.9091\n",
      "Epoch 43/120\n",
      "82732/82732 [==============================] - 1s 12us/step - loss: 2.1160 - KL_loss_layer_loss: 0.6996 - reconstructed_probability_loss: 0.8568 - val_loss: 2.0562 - val_KL_loss_layer_loss: 0.6868 - val_reconstructed_probability_loss: 0.8200\n",
      "Epoch 44/120\n",
      "82732/82732 [==============================] - 1s 12us/step - loss: 1.9956 - KL_loss_layer_loss: 0.6610 - reconstructed_probability_loss: 0.8059 - val_loss: 1.8487 - val_KL_loss_layer_loss: 0.6390 - val_reconstructed_probability_loss: 0.6984\n",
      "Epoch 45/120\n",
      "82732/82732 [==============================] - 1s 12us/step - loss: 1.8360 - KL_loss_layer_loss: 0.6143 - reconstructed_probability_loss: 0.7302 - val_loss: 1.8267 - val_KL_loss_layer_loss: 0.5796 - val_reconstructed_probability_loss: 0.7834\n",
      "Epoch 46/120\n",
      "82732/82732 [==============================] - 1s 11us/step - loss: 1.7250 - KL_loss_layer_loss: 0.5449 - reconstructed_probability_loss: 0.7442 - val_loss: 1.7877 - val_KL_loss_layer_loss: 0.5273 - val_reconstructed_probability_loss: 0.8386\n",
      "Epoch 47/120\n",
      "82732/82732 [==============================] - 1s 12us/step - loss: 1.6029 - KL_loss_layer_loss: 0.4966 - reconstructed_probability_loss: 0.7091 - val_loss: 1.5143 - val_KL_loss_layer_loss: 0.4605 - val_reconstructed_probability_loss: 0.6854\n",
      "Epoch 48/120\n",
      "82732/82732 [==============================] - 1s 12us/step - loss: 1.4727 - KL_loss_layer_loss: 0.4434 - reconstructed_probability_loss: 0.6747 - val_loss: 1.3699 - val_KL_loss_layer_loss: 0.4183 - val_reconstructed_probability_loss: 0.6170\n",
      "Epoch 49/120\n",
      "82732/82732 [==============================] - 1s 12us/step - loss: 1.3416 - KL_loss_layer_loss: 0.4084 - reconstructed_probability_loss: 0.6065 - val_loss: 1.5054 - val_KL_loss_layer_loss: 0.4073 - val_reconstructed_probability_loss: 0.7723\n",
      "Epoch 50/120\n",
      "82732/82732 [==============================] - 1s 12us/step - loss: 1.2682 - KL_loss_layer_loss: 0.3683 - reconstructed_probability_loss: 0.6053 - val_loss: 1.1339 - val_KL_loss_layer_loss: 0.3554 - val_reconstructed_probability_loss: 0.4943\n",
      "Epoch 51/120\n",
      "82732/82732 [==============================] - 1s 12us/step - loss: 1.1187 - KL_loss_layer_loss: 0.3260 - reconstructed_probability_loss: 0.5319 - val_loss: 1.0899 - val_KL_loss_layer_loss: 0.3127 - val_reconstructed_probability_loss: 0.5269\n",
      "Epoch 52/120\n",
      "82732/82732 [==============================] - 1s 12us/step - loss: 1.0478 - KL_loss_layer_loss: 0.2918 - reconstructed_probability_loss: 0.5225 - val_loss: 0.9996 - val_KL_loss_layer_loss: 0.2425 - val_reconstructed_probability_loss: 0.5631\n",
      "Epoch 53/120\n",
      "82732/82732 [==============================] - 1s 12us/step - loss: 0.9809 - KL_loss_layer_loss: 0.2225 - reconstructed_probability_loss: 0.5804 - val_loss: 0.9008 - val_KL_loss_layer_loss: 0.1841 - val_reconstructed_probability_loss: 0.5694\n",
      "Epoch 54/120\n",
      "82732/82732 [==============================] - 1s 12us/step - loss: 0.8851 - KL_loss_layer_loss: 0.1569 - reconstructed_probability_loss: 0.6026 - val_loss: 0.7863 - val_KL_loss_layer_loss: 0.1172 - val_reconstructed_probability_loss: 0.5754\n",
      "Epoch 55/120\n",
      "82732/82732 [==============================] - 1s 12us/step - loss: 0.7989 - KL_loss_layer_loss: 0.0966 - reconstructed_probability_loss: 0.6251 - val_loss: 0.6825 - val_KL_loss_layer_loss: 0.0832 - val_reconstructed_probability_loss: 0.5327\n",
      "Epoch 56/120\n",
      "82732/82732 [==============================] - 1s 12us/step - loss: 0.6869 - KL_loss_layer_loss: 0.0731 - reconstructed_probability_loss: 0.5554 - val_loss: 0.6181 - val_KL_loss_layer_loss: 0.0371 - val_reconstructed_probability_loss: 0.5514\n",
      "Epoch 57/120\n",
      "82732/82732 [==============================] - 1s 12us/step - loss: 0.6480 - KL_loss_layer_loss: 0.0283 - reconstructed_probability_loss: 0.5969 - val_loss: 0.5344 - val_KL_loss_layer_loss: -0.0110 - val_reconstructed_probability_loss: 0.5542\n",
      "Epoch 58/120\n",
      "82732/82732 [==============================] - 1s 12us/step - loss: 0.5257 - KL_loss_layer_loss: -0.0228 - reconstructed_probability_loss: 0.5667 - val_loss: 0.4260 - val_KL_loss_layer_loss: -0.0389 - val_reconstructed_probability_loss: 0.4961\n",
      "Epoch 59/120\n",
      "82732/82732 [==============================] - 1s 11us/step - loss: 0.4672 - KL_loss_layer_loss: -0.0488 - reconstructed_probability_loss: 0.5551 - val_loss: 0.3980 - val_KL_loss_layer_loss: -0.0688 - val_reconstructed_probability_loss: 0.5219\n",
      "Epoch 60/120\n",
      "82732/82732 [==============================] - 1s 12us/step - loss: 0.4168 - KL_loss_layer_loss: -0.1090 - reconstructed_probability_loss: 0.6129 - val_loss: 0.2946 - val_KL_loss_layer_loss: -0.1432 - val_reconstructed_probability_loss: 0.5523\n",
      "Epoch 61/120\n",
      "82732/82732 [==============================] - 1s 12us/step - loss: 0.3201 - KL_loss_layer_loss: -0.1522 - reconstructed_probability_loss: 0.5942 - val_loss: 0.2134 - val_KL_loss_layer_loss: -0.1860 - val_reconstructed_probability_loss: 0.5481\n",
      "Epoch 62/120\n",
      "82732/82732 [==============================] - 1s 12us/step - loss: 0.2827 - KL_loss_layer_loss: -0.2120 - reconstructed_probability_loss: 0.6642 - val_loss: 0.3375 - val_KL_loss_layer_loss: -0.2322 - val_reconstructed_probability_loss: 0.7555\n",
      "Epoch 63/120\n",
      "82732/82732 [==============================] - 1s 12us/step - loss: 0.2112 - KL_loss_layer_loss: -0.2515 - reconstructed_probability_loss: 0.6639 - val_loss: 0.4302 - val_KL_loss_layer_loss: -0.2769 - val_reconstructed_probability_loss: 0.9287\n",
      "Epoch 64/120\n",
      "82732/82732 [==============================] - 1s 13us/step - loss: 0.1334 - KL_loss_layer_loss: -0.3114 - reconstructed_probability_loss: 0.6940 - val_loss: 0.0226 - val_KL_loss_layer_loss: -0.3208 - val_reconstructed_probability_loss: 0.6000\n",
      "Epoch 65/120\n",
      "82732/82732 [==============================] - 1s 12us/step - loss: 0.0678 - KL_loss_layer_loss: -0.3448 - reconstructed_probability_loss: 0.6884 - val_loss: -0.0278 - val_KL_loss_layer_loss: -0.3693 - val_reconstructed_probability_loss: 0.6370\n",
      "Epoch 66/120\n",
      "82732/82732 [==============================] - 1s 12us/step - loss: 0.0655 - KL_loss_layer_loss: -0.3878 - reconstructed_probability_loss: 0.7636 - val_loss: -0.0341 - val_KL_loss_layer_loss: -0.4684 - val_reconstructed_probability_loss: 0.8091\n",
      "Epoch 67/120\n",
      "82732/82732 [==============================] - 1s 12us/step - loss: -0.0082 - KL_loss_layer_loss: -0.4845 - reconstructed_probability_loss: 0.8639 - val_loss: -0.1364 - val_KL_loss_layer_loss: -0.5082 - val_reconstructed_probability_loss: 0.7784\n",
      "Epoch 68/120\n",
      "82732/82732 [==============================] - 1s 13us/step - loss: -0.1352 - KL_loss_layer_loss: -0.5166 - reconstructed_probability_loss: 0.7947 - val_loss: -0.1348 - val_KL_loss_layer_loss: -0.5113 - val_reconstructed_probability_loss: 0.7856\n",
      "Epoch 69/120\n",
      "82732/82732 [==============================] - 1s 13us/step - loss: -0.1374 - KL_loss_layer_loss: -0.5415 - reconstructed_probability_loss: 0.8373 - val_loss: -0.1397 - val_KL_loss_layer_loss: -0.5637 - val_reconstructed_probability_loss: 0.8749\n",
      "Epoch 70/120\n",
      "82732/82732 [==============================] - 1s 13us/step - loss: -0.2080 - KL_loss_layer_loss: -0.5828 - reconstructed_probability_loss: 0.8411 - val_loss: -0.2598 - val_KL_loss_layer_loss: -0.5936 - val_reconstructed_probability_loss: 0.8088\n",
      "Epoch 71/120\n",
      "82732/82732 [==============================] - 1s 12us/step - loss: -0.2151 - KL_loss_layer_loss: -0.6402 - reconstructed_probability_loss: 0.9372 - val_loss: -0.2372 - val_KL_loss_layer_loss: -0.6860 - val_reconstructed_probability_loss: 0.9975\n",
      "Epoch 72/120\n",
      "82732/82732 [==============================] - 1s 12us/step - loss: -0.3027 - KL_loss_layer_loss: -0.6911 - reconstructed_probability_loss: 0.9413 - val_loss: -0.2827 - val_KL_loss_layer_loss: -0.7071 - val_reconstructed_probability_loss: 0.9900\n",
      "Epoch 73/120\n",
      "82732/82732 [==============================] - 1s 13us/step - loss: -0.3225 - KL_loss_layer_loss: -0.7445 - reconstructed_probability_loss: 1.0177 - val_loss: -0.4311 - val_KL_loss_layer_loss: -0.7826 - val_reconstructed_probability_loss: 0.9775\n",
      "Epoch 74/120\n",
      "82732/82732 [==============================] - 1s 12us/step - loss: -0.4222 - KL_loss_layer_loss: -0.7853 - reconstructed_probability_loss: 0.9912 - val_loss: -0.3247 - val_KL_loss_layer_loss: -0.7901 - val_reconstructed_probability_loss: 1.0975\n",
      "Epoch 75/120\n",
      "82732/82732 [==============================] - 1s 13us/step - loss: -0.4317 - KL_loss_layer_loss: -0.8093 - reconstructed_probability_loss: 1.0251 - val_loss: -0.5239 - val_KL_loss_layer_loss: -0.8494 - val_reconstructed_probability_loss: 1.0050\n",
      "Epoch 76/120\n",
      "82732/82732 [==============================] - 1s 13us/step - loss: -0.4568 - KL_loss_layer_loss: -0.8529 - reconstructed_probability_loss: 1.0784 - val_loss: -0.3304 - val_KL_loss_layer_loss: -0.9202 - val_reconstructed_probability_loss: 1.3261\n",
      "Epoch 77/120\n",
      "82732/82732 [==============================] - 1s 12us/step - loss: -0.5416 - KL_loss_layer_loss: -0.9143 - reconstructed_probability_loss: 1.1040 - val_loss: -0.4345 - val_KL_loss_layer_loss: -0.9023 - val_reconstructed_probability_loss: 1.1895\n",
      "Epoch 78/120\n",
      "82732/82732 [==============================] - 1s 12us/step - loss: -0.5677 - KL_loss_layer_loss: -0.9254 - reconstructed_probability_loss: 1.0980 - val_loss: -0.6422 - val_KL_loss_layer_loss: -0.9342 - val_reconstructed_probability_loss: 1.0394\n",
      "Epoch 79/120\n",
      "82732/82732 [==============================] - 1s 12us/step - loss: -0.5679 - KL_loss_layer_loss: -0.9657 - reconstructed_probability_loss: 1.1704 - val_loss: -0.3759 - val_KL_loss_layer_loss: -1.0027 - val_reconstructed_probability_loss: 1.4291\n",
      "Epoch 80/120\n",
      "82732/82732 [==============================] - 1s 12us/step - loss: -0.6270 - KL_loss_layer_loss: -1.0785 - reconstructed_probability_loss: 1.3143 - val_loss: -0.6221 - val_KL_loss_layer_loss: -1.0933 - val_reconstructed_probability_loss: 1.3458\n",
      "Epoch 81/120\n",
      "82732/82732 [==============================] - 1s 12us/step - loss: -0.7123 - KL_loss_layer_loss: -1.0699 - reconstructed_probability_loss: 1.2135 - val_loss: -0.7048 - val_KL_loss_layer_loss: -1.0791 - val_reconstructed_probability_loss: 1.2375\n",
      "Epoch 82/120\n",
      "82732/82732 [==============================] - 1s 12us/step - loss: -0.7372 - KL_loss_layer_loss: -1.0702 - reconstructed_probability_loss: 1.1893 - val_loss: -0.8253 - val_KL_loss_layer_loss: -1.0889 - val_reconstructed_probability_loss: 1.1348\n",
      "Epoch 83/120\n",
      "82732/82732 [==============================] - 1s 12us/step - loss: -0.7255 - KL_loss_layer_loss: -1.1438 - reconstructed_probability_loss: 1.3333 - val_loss: -0.8782 - val_KL_loss_layer_loss: -1.1782 - val_reconstructed_probability_loss: 1.2426\n",
      "Epoch 84/120\n",
      "82732/82732 [==============================] - 1s 12us/step - loss: -0.8061 - KL_loss_layer_loss: -1.1638 - reconstructed_probability_loss: 1.2888 - val_loss: -0.9242 - val_KL_loss_layer_loss: -1.1919 - val_reconstructed_probability_loss: 1.2211\n",
      "Epoch 85/120\n",
      "82732/82732 [==============================] - 1s 12us/step - loss: -0.8377 - KL_loss_layer_loss: -1.1967 - reconstructed_probability_loss: 1.3163 - val_loss: -0.9505 - val_KL_loss_layer_loss: -1.2302 - val_reconstructed_probability_loss: 1.2639\n",
      "Epoch 86/120\n",
      "82732/82732 [==============================] - 1s 12us/step - loss: -0.8738 - KL_loss_layer_loss: -1.2434 - reconstructed_probability_loss: 1.3643 - val_loss: -0.8810 - val_KL_loss_layer_loss: -1.2598 - val_reconstructed_probability_loss: 1.3867\n",
      "Epoch 87/120\n",
      "82732/82732 [==============================] - 1s 12us/step - loss: -0.9209 - KL_loss_layer_loss: -1.2404 - reconstructed_probability_loss: 1.3119 - val_loss: -0.9491 - val_KL_loss_layer_loss: -1.2451 - val_reconstructed_probability_loss: 1.2922\n",
      "Epoch 88/120\n",
      "82732/82732 [==============================] - 1s 12us/step - loss: -0.9112 - KL_loss_layer_loss: -1.3025 - reconstructed_probability_loss: 1.4334 - val_loss: -0.9402 - val_KL_loss_layer_loss: -1.3142 - val_reconstructed_probability_loss: 1.4254\n",
      "Epoch 89/120\n",
      "82732/82732 [==============================] - 1s 12us/step - loss: -0.9501 - KL_loss_layer_loss: -1.3317 - reconstructed_probability_loss: 1.4470 - val_loss: -0.8460 - val_KL_loss_layer_loss: -1.3575 - val_reconstructed_probability_loss: 1.5975\n",
      "Epoch 90/120\n",
      "82732/82732 [==============================] - 1s 12us/step - loss: -0.9887 - KL_loss_layer_loss: -1.3708 - reconstructed_probability_loss: 1.4787 - val_loss: -0.9403 - val_KL_loss_layer_loss: -1.3963 - val_reconstructed_probability_loss: 1.5731\n",
      "Epoch 91/120\n",
      "82732/82732 [==============================] - 1s 13us/step - loss: -1.0157 - KL_loss_layer_loss: -1.4050 - reconstructed_probability_loss: 1.5133 - val_loss: -0.8937 - val_KL_loss_layer_loss: -1.4315 - val_reconstructed_probability_loss: 1.6830\n",
      "Epoch 92/120\n",
      "82732/82732 [==============================] - 1s 13us/step - loss: -1.1697 - KL_loss_layer_loss: -1.4177 - reconstructed_probability_loss: 1.3821 - val_loss: -1.1812 - val_KL_loss_layer_loss: -1.4118 - val_reconstructed_probability_loss: 1.3600\n",
      "Epoch 93/120\n",
      "34500/82732 [===========>..................] - ETA: 0s - loss: -1.2049 - KL_loss_layer_loss: -1.4095 - reconstructed_probability_loss: 1.3322"
     ]
    }
   ],
   "source": [
    "train = True\n",
    "if train:\n",
    "    vae = instance.train(\n",
    "        x_train=train_norm.data,\n",
    "        x_test=test_norm.data,\n",
    "        y_train=[np.empty((train_norm.shape[0], neck_dim)), np.empty_like(train_norm.data)],\n",
    "        y_test=[np.empty((test_norm.shape[0], neck_dim)), np.empty_like(test_norm.data)],\n",
    "        optimizer=\"adam\",\n",
    "        loss=[Identity, Identity],\n",
    "        loss_weights=[0., 1.0],\n",
    "        epochs=20,\n",
    "        model=vae,\n",
    "        use_callbacks=True,\n",
    "        learning_rate=0.0005,\n",
    "        custom_objects=custom_objects,\n",
    "        batch_size=500\n",
    "    )\n",
    "    vae = instance.train(\n",
    "        x_train=train_norm.data,\n",
    "        x_test=test_norm.data,\n",
    "        y_train=[np.empty((train_norm.shape[0], neck_dim)), np.empty_like(train_norm.data)],\n",
    "        y_test=[np.empty((test_norm.shape[0], neck_dim)), np.empty_like(test_norm.data)],\n",
    "        optimizer=\"adam\",\n",
    "        loss=[Identity, Identity],\n",
    "        loss_weights=[1.8, 1.0],\n",
    "        epochs=100,\n",
    "        model=vae,\n",
    "        use_callbacks=True,\n",
    "        learning_rate=0.0005,\n",
    "        custom_objects=custom_objects,\n",
    "        batch_size=500\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training progress\n",
    "Obviously, KL loss was not weighted properly in this attempt. We won't mind that now, though. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "instance.plot_metrics(fnmatch_criteria=\"*?????loss\", figloc=(.52,.565), figsize=(12,8), figname='Training Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "z_mu, z_sigma = vae.get_layer(\"z_mu\").output, vae.get_layer(\"z_sigma\").output\n",
    "input_layer = vae.get_layer(\"input\").output\n",
    "\n",
    "z_out = vae.get_layer(\"latent_sampling\").output\n",
    "out_mu, out_sigma = vae.get_layer(\"output_mu\").output, vae.get_layer(\"output_sigma\").output\n",
    "\n",
    "encoder = keras.models.Model(inputs=input_layer, outputs=[z_mu, z_sigma])\n",
    "autoencoder = keras.models.Model(inputs=input_layer, outputs=[out_mu, out_sigma])\n",
    "zencoder = keras.models.Model(inputs=input_layer, outputs=z_out)\n",
    "\n",
    "# aux = encoder.predict(np.zeros((1,input_dim)))\n",
    "# mu_prior = aux[0][0]\n",
    "# sigma_prior = aux[1][0]\n",
    "\n",
    "# mu_prior, sigma_prior\n",
    "\n",
    "inputs_out = [data_norm, signal_norm]\n",
    "outputs_out = [autoencoder.predict(data_norm.data), autoencoder.predict(signal_norm.data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "utils.plot_spdfs(inputs_out, outputs_out, figname=\"output distributions\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "signal_jets_norm, data_jets_norm = [\n",
    "    [train_raw.norm(j.cfilter(signal_norm.headers), **norm_args) for j in jet_table] for jet_table in [signal_jets, data_jets]\n",
    "]\n",
    "\n",
    "nodenames = ['node {}'.format(i) for i in range(neck_dim)]\n",
    "\n",
    "data_reps, signal_reps = (\n",
    "    utils.data_table(zencoder.predict(data_norm.data), headers=nodenames, name=\"background_reps\"),\n",
    "    utils.data_table(zencoder.predict(signal_norm.data), headers=nodenames, name=\"signal_reps\")\n",
    ")\n",
    "\n",
    "data_jet_reps, signal_jet_reps = (\n",
    "    [utils.data_table(zencoder.predict(d.data), headers=nodenames, name=\"jet {} background reps\".format(i)) for i,d in enumerate(data_jets_norm)],\n",
    "    [utils.data_table(zencoder.predict(d.data), headers=nodenames, name=\"jet {} signal reps\".format(i)) for i,d in enumerate(signal_jets_norm)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# node_outputs[0].plot(node_outputs[1], normed=1, figname='node outputs', figloc='upper right', figsize=15, fontsize=20, cols=2)\n",
    "signal_jet_reps[0].plot(\n",
    "    signal_jet_reps[1:] + data_jet_reps, \n",
    "    normed=1, figname='node outputs', figloc='upper right', \n",
    "    figsize=20, fontsize=20, cols=3, bins=60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "err_data = utils.data_table(vae.predict(data_norm.data)[1], headers=['loss'], name='combined data error')\n",
    "err_signal = utils.data_table(vae.predict(signal_norm.data)[1], headers=['loss'], name='combined signal error')\n",
    "err_data_jets = [\n",
    "    utils.data_table(\n",
    "        vae.predict(d.data)[1],\n",
    "        headers=['loss'],\n",
    "        name='jet {} background error'.format(i + 1)\n",
    "    ) for i,d in enumerate(data_jets_norm)\n",
    "]\n",
    "\n",
    "err_signal_jets = [\n",
    "    utils.data_table(\n",
    "        vae.predict(d.data)[1],\n",
    "        headers=['loss'],\n",
    "        name='jet {} signal error'.format(i + 1)\n",
    "    ) for i,d in enumerate(signal_jets_norm)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "err_signal_jets[0].plot(\n",
    "    err_signal_jets[1:] + err_data_jets, bins=100, normed=1, yscale='linear',\n",
    "    figsize=(10,10), alpha=1.5, rng=(-6, 20), figloc=\"upper right\",\n",
    "    figname=\"error\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "utils.roc_auc_plot([err_data], err_signal_jets + [err_signal], ticksize=10, yscale='linear', figsize=10, figloc=(.4, .10), figname=\"ROC by jet #, compared against all background jets\")\n",
    "utils.roc_auc_plot(err_data_jets + [err_data], err_signal_jets + [err_signal], ticksize=10, yscale='linear', figsize=10, figloc=(.4, .10), figname=\"ROC by jet #, compared against respective background jet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# def plot_vae_necknodes(encoder, x_ins, bins=50, *args, **kwargs):\n",
    "    \n",
    "#     input_dim = encoder.layers[0].get_input_shape_at(0)[1]\n",
    "#     aux = encoder.predict(np.zeros((1, input_dim)))\n",
    "#     mu_prior = aux[0][0]\n",
    "#     sigma_prior = aux[1][0]\n",
    "    \n",
    "#     to_calculate = [encoder.predict(d.data) for d in x_ins] \n",
    "    \n",
    "#     assert len(mu_prior) == len(sigma_prior)\n",
    "#     neck_dim = len(mu_prior)\n",
    "    \n",
    "#     fig, ax_on, ax_off, plt_end, colors = get_plot_params(neck_dim, *args, **kwargs)\n",
    "    \n",
    "#     for i in range(neck_dim):\n",
    "#         ax = ax_on(i)\n",
    "#         lower = mu_prior[i] - 6*sigma_prior[i]\n",
    "#         upper = mu_prior[i] + 6*sigma_prior[i]\n",
    "\n",
    "#         width = (upper - lower) / bins\n",
    "#         z = np.arange(lower, upper, width)\n",
    "        \n",
    "#         for j,calc in enumerate(to_calculate):\n",
    "            \n",
    "#             mus, sigmas = calc\n",
    "#             content = sum_of_gaussians(z, mus[i], sigmas[i])*width\n",
    "#             ax.errorbar(z + width/2., content, xerr=width/2, c=colors[j], label=x_ins[j].name)\n",
    "        \n",
    "#         ax_off(\"node {}\".format(i))\n",
    "#     plt_end()\n",
    "        \n",
    "# plot_vae_necknodes(encoder, inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
